{"run_name": "output/xlingual/tk-instruct-3b-def-pos", "predict_loss": 0.5213112235069275, "predict_exact_match": 40.2857, "predict_rouge1": 46.021, "predict_rougeL": 46.021, "predict_exact_match_for_task1561_clickbait_new_bg_summarization": 0.0, "predict_rouge1_for_task1561_clickbait_new_bg_summarization": 6.5994, "predict_rougeL_for_task1561_clickbait_new_bg_summarization": 6.5994, "predict_exact_match_for_task969_xcopa_commonsense_cause_effect_et": 51.0, "predict_rouge1_for_task969_xcopa_commonsense_cause_effect_et": 51.0, "predict_rougeL_for_task969_xcopa_commonsense_cause_effect_et": 51.0, "predict_exact_match_for_task463_parsinlu_entailment_classification": 32.0, "predict_rouge1_for_task463_parsinlu_entailment_classification": 32.0, "predict_rougeL_for_task463_parsinlu_entailment_classification": 32.0, "predict_exact_match_for_task1627_copa_hr_classification": 51.0, "predict_rouge1_for_task1627_copa_hr_classification": 51.0, "predict_rougeL_for_task1627_copa_hr_classification": 51.0, "predict_exact_match_for_task396_persianqa_classification": 50.0, "predict_rouge1_for_task396_persianqa_classification": 50.0, "predict_rougeL_for_task396_persianqa_classification": 50.0, "predict_exact_match_for_task1177_xcopa_commonsense_cause_effect_ta": 50.0, "predict_rouge1_for_task1177_xcopa_commonsense_cause_effect_ta": 50.0, "predict_rougeL_for_task1177_xcopa_commonsense_cause_effect_ta": 50.0, "predict_exact_match_for_task1182_xcopa_commonsense_reasoning_vi": 49.0, "predict_rouge1_for_task1182_xcopa_commonsense_reasoning_vi": 49.0, "predict_rougeL_for_task1182_xcopa_commonsense_reasoning_vi": 49.0, "predict_exact_match_for_task942_copa_mr_commonsense_reasoning": 0.0, "predict_rouge1_for_task942_copa_mr_commonsense_reasoning": 50.0, "predict_rougeL_for_task942_copa_mr_commonsense_reasoning": 50.0, "predict_exact_match_for_task1628_copa_hr_question_answering": 6.0, "predict_rouge1_for_task1628_copa_hr_question_answering": 27.4649, "predict_rougeL_for_task1628_copa_hr_question_answering": 27.4649, "predict_exact_match_for_task941_copa_gu_commonsense_cause_effect": 50.0, "predict_rouge1_for_task941_copa_gu_commonsense_cause_effect": 50.0, "predict_rougeL_for_task941_copa_gu_commonsense_cause_effect": 50.0, "predict_exact_match_for_task1184_xcopa_commonsense_reasoning_zh": 50.0, "predict_rouge1_for_task1184_xcopa_commonsense_reasoning_zh": 50.0, "predict_rougeL_for_task1184_xcopa_commonsense_reasoning_zh": 50.0, "predict_exact_match_for_task1170_xcopa_commonsense_reasoning_id": 60.0, "predict_rouge1_for_task1170_xcopa_commonsense_reasoning_id": 60.0, "predict_rougeL_for_task1170_xcopa_commonsense_reasoning_id": 60.0, "predict_exact_match_for_task1171_xcopa_commonsense_cause_effect_id": 51.0, "predict_rouge1_for_task1171_xcopa_commonsense_cause_effect_id": 51.0, "predict_rougeL_for_task1171_xcopa_commonsense_cause_effect_id": 51.0, "predict_exact_match_for_task939_copa_hi_commonsense_cause_effect": 50.0, "predict_rouge1_for_task939_copa_hi_commonsense_cause_effect": 50.0, "predict_rougeL_for_task939_copa_hi_commonsense_cause_effect": 50.0, "predict_exact_match_for_task1185_xcopa_commonsense_cause_effect_zh": 50.0, "predict_rouge1_for_task1185_xcopa_commonsense_cause_effect_zh": 50.0, "predict_rougeL_for_task1185_xcopa_commonsense_cause_effect_zh": 50.0, "predict_exact_match_for_task1179_xcopa_commonsense_cause_effect_th": 50.0, "predict_rouge1_for_task1179_xcopa_commonsense_cause_effect_th": 50.0, "predict_rougeL_for_task1179_xcopa_commonsense_cause_effect_th": 50.0, "predict_exact_match_for_task1174_xcopa_commonsense_reasoning_sw": 50.0, "predict_rouge1_for_task1174_xcopa_commonsense_reasoning_sw": 50.0, "predict_rougeL_for_task1174_xcopa_commonsense_reasoning_sw": 50.0, "predict_exact_match_for_task1175_xcopa_commonsense_cause_effect_sw": 50.0, "predict_rouge1_for_task1175_xcopa_commonsense_cause_effect_sw": 50.0, "predict_rougeL_for_task1175_xcopa_commonsense_cause_effect_sw": 50.0, "predict_exact_match_for_task943_copa_mr_commonsense_cause_effect": 50.0, "predict_rouge1_for_task943_copa_mr_commonsense_cause_effect": 50.0, "predict_rougeL_for_task943_copa_mr_commonsense_cause_effect": 50.0, "predict_exact_match_for_task1168_xcopa_commonsense_reasoning_ht": 47.0, "predict_rouge1_for_task1168_xcopa_commonsense_reasoning_ht": 47.0, "predict_rougeL_for_task1168_xcopa_commonsense_reasoning_ht": 47.0, "predict_exact_match_for_task1181_xcopa_commonsense_cause_effect_tr": 50.0, "predict_rouge1_for_task1181_xcopa_commonsense_cause_effect_tr": 50.0, "predict_rougeL_for_task1181_xcopa_commonsense_cause_effect_tr": 50.0, "predict_exact_match_for_task1180_xcopa_commonsense_reasoning_tr": 49.0, "predict_rouge1_for_task1180_xcopa_commonsense_reasoning_tr": 49.0, "predict_rougeL_for_task1180_xcopa_commonsense_reasoning_tr": 49.0, "predict_exact_match_for_task968_xcopa_commonsense_reasoning_et": 52.0, "predict_rouge1_for_task968_xcopa_commonsense_reasoning_et": 52.0, "predict_rougeL_for_task968_xcopa_commonsense_reasoning_et": 52.0, "predict_exact_match_for_task1172_xcopa_commonsense_reasoning_it": 56.0, "predict_rouge1_for_task1172_xcopa_commonsense_reasoning_it": 56.0, "predict_rougeL_for_task1172_xcopa_commonsense_reasoning_it": 56.0, "predict_exact_match_for_task938_copa_hi_commonsense_reasoning": 0.0, "predict_rouge1_for_task938_copa_hi_commonsense_reasoning": 50.0, "predict_rougeL_for_task938_copa_hi_commonsense_reasoning": 50.0, "predict_exact_match_for_task1173_xcopa_commonsense_cause_effect_it": 52.0, "predict_rouge1_for_task1173_xcopa_commonsense_cause_effect_it": 52.0, "predict_rougeL_for_task1173_xcopa_commonsense_cause_effect_it": 52.0, "predict_exact_match_for_task1629_copa_hr_classification": 50.0, "predict_rouge1_for_task1629_copa_hr_classification": 50.0, "predict_rougeL_for_task1629_copa_hr_classification": 50.0, "predict_exact_match_for_task1626_copa_hr_question_answering": 19.0, "predict_rouge1_for_task1626_copa_hr_question_answering": 42.6709, "predict_rougeL_for_task1626_copa_hr_question_answering": 42.6709, "predict_exact_match_for_task1176_xcopa_commonsense_reasoning_ta": 50.0, "predict_rouge1_for_task1176_xcopa_commonsense_reasoning_ta": 50.0, "predict_rougeL_for_task1176_xcopa_commonsense_reasoning_ta": 50.0, "predict_exact_match_for_task1183_xcopa_commonsense_cause_effect_vi": 50.0, "predict_rouge1_for_task1183_xcopa_commonsense_cause_effect_vi": 50.0, "predict_rougeL_for_task1183_xcopa_commonsense_cause_effect_vi": 50.0, "predict_exact_match_for_task464_parsinlu_entailment_sentence_generation": 1.0, "predict_rouge1_for_task464_parsinlu_entailment_sentence_generation": 0.0, "predict_rougeL_for_task464_parsinlu_entailment_sentence_generation": 0.0, "predict_exact_match_for_task940_copa_gu_commonsense_reasoning": 0.0, "predict_rouge1_for_task940_copa_gu_commonsense_reasoning": 50.0, "predict_rougeL_for_task940_copa_gu_commonsense_reasoning": 50.0, "predict_exact_match_for_task1169_xcopa_commonsense_cause_effect_ht": 50.0, "predict_rouge1_for_task1169_xcopa_commonsense_cause_effect_ht": 50.0, "predict_rougeL_for_task1169_xcopa_commonsense_cause_effect_ht": 50.0, "predict_exact_match_for_task534_farstail_entailment": 34.0, "predict_rouge1_for_task534_farstail_entailment": 34.0, "predict_rougeL_for_task534_farstail_entailment": 34.0, "predict_exact_match_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_rouge1_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_rougeL_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_exact_match_for_title_generation": 0.0, "predict_rouge1_for_title_generation": 6.5994, "predict_rougeL_for_title_generation": 6.5994, "predict_exact_match_for_cause_effect_classification": 43.1, "predict_rouge1_for_cause_effect_classification": 49.6045, "predict_rougeL_for_cause_effect_classification": 49.6045, "predict_exact_match_for_textual_entailment": 22.3333, "predict_rouge1_for_textual_entailment": 22.0, "predict_rougeL_for_textual_entailment": 22.0, "predict_exact_match_for_answerability_classification": 50.0, "predict_rouge1_for_answerability_classification": 50.0, "predict_rougeL_for_answerability_classification": 50.0, "predict_gen_len": 9.4629, "predict_global_step": 0, "predict_runtime": 2126.4838, "predict_samples_per_second": 1.646, "predict_steps_per_second": 1.646, "predict_samples": 3500}