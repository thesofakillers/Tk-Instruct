{"run_name": "output/xlingual/mtk-instruct-3b-def-pos", "predict_loss": 0.5069992542266846, "predict_exact_match": 50.0, "predict_rouge1": 51.7386, "predict_rougeL": 51.7304, "predict_exact_match_for_task1561_clickbait_new_bg_summarization": 3.0, "predict_rouge1_for_task1561_clickbait_new_bg_summarization": 9.3905, "predict_rougeL_for_task1561_clickbait_new_bg_summarization": 9.1048, "predict_exact_match_for_task969_xcopa_commonsense_cause_effect_et": 60.0, "predict_rouge1_for_task969_xcopa_commonsense_cause_effect_et": 60.0, "predict_rougeL_for_task969_xcopa_commonsense_cause_effect_et": 60.0, "predict_exact_match_for_task463_parsinlu_entailment_classification": 21.0, "predict_rouge1_for_task463_parsinlu_entailment_classification": 43.0, "predict_rougeL_for_task463_parsinlu_entailment_classification": 43.0, "predict_exact_match_for_task1627_copa_hr_classification": 59.0, "predict_rouge1_for_task1627_copa_hr_classification": 59.0, "predict_rougeL_for_task1627_copa_hr_classification": 59.0, "predict_exact_match_for_task396_persianqa_classification": 56.0, "predict_rouge1_for_task396_persianqa_classification": 56.0, "predict_rougeL_for_task396_persianqa_classification": 56.0, "predict_exact_match_for_task1177_xcopa_commonsense_cause_effect_ta": 45.0, "predict_rouge1_for_task1177_xcopa_commonsense_cause_effect_ta": 45.0, "predict_rougeL_for_task1177_xcopa_commonsense_cause_effect_ta": 45.0, "predict_exact_match_for_task1182_xcopa_commonsense_reasoning_vi": 52.0, "predict_rouge1_for_task1182_xcopa_commonsense_reasoning_vi": 52.0, "predict_rougeL_for_task1182_xcopa_commonsense_reasoning_vi": 52.0, "predict_exact_match_for_task942_copa_mr_commonsense_reasoning": 58.0, "predict_rouge1_for_task942_copa_mr_commonsense_reasoning": 58.0, "predict_rougeL_for_task942_copa_mr_commonsense_reasoning": 58.0, "predict_exact_match_for_task1628_copa_hr_question_answering": 38.0, "predict_rouge1_for_task1628_copa_hr_question_answering": 50.3925, "predict_rougeL_for_task1628_copa_hr_question_answering": 50.3925, "predict_exact_match_for_task941_copa_gu_commonsense_cause_effect": 53.0, "predict_rouge1_for_task941_copa_gu_commonsense_cause_effect": 53.0, "predict_rougeL_for_task941_copa_gu_commonsense_cause_effect": 53.0, "predict_exact_match_for_task1184_xcopa_commonsense_reasoning_zh": 53.0, "predict_rouge1_for_task1184_xcopa_commonsense_reasoning_zh": 53.0, "predict_rougeL_for_task1184_xcopa_commonsense_reasoning_zh": 53.0, "predict_exact_match_for_task1170_xcopa_commonsense_reasoning_id": 54.0, "predict_rouge1_for_task1170_xcopa_commonsense_reasoning_id": 54.0, "predict_rougeL_for_task1170_xcopa_commonsense_reasoning_id": 54.0, "predict_exact_match_for_task1171_xcopa_commonsense_cause_effect_id": 55.0, "predict_rouge1_for_task1171_xcopa_commonsense_cause_effect_id": 55.0, "predict_rougeL_for_task1171_xcopa_commonsense_cause_effect_id": 55.0, "predict_exact_match_for_task939_copa_hi_commonsense_cause_effect": 55.0, "predict_rouge1_for_task939_copa_hi_commonsense_cause_effect": 55.0, "predict_rougeL_for_task939_copa_hi_commonsense_cause_effect": 55.0, "predict_exact_match_for_task1185_xcopa_commonsense_cause_effect_zh": 49.0, "predict_rouge1_for_task1185_xcopa_commonsense_cause_effect_zh": 49.0, "predict_rougeL_for_task1185_xcopa_commonsense_cause_effect_zh": 49.0, "predict_exact_match_for_task1179_xcopa_commonsense_cause_effect_th": 54.0, "predict_rouge1_for_task1179_xcopa_commonsense_cause_effect_th": 54.0, "predict_rougeL_for_task1179_xcopa_commonsense_cause_effect_th": 54.0, "predict_exact_match_for_task1174_xcopa_commonsense_reasoning_sw": 51.0, "predict_rouge1_for_task1174_xcopa_commonsense_reasoning_sw": 51.0, "predict_rougeL_for_task1174_xcopa_commonsense_reasoning_sw": 51.0, "predict_exact_match_for_task1175_xcopa_commonsense_cause_effect_sw": 60.0, "predict_rouge1_for_task1175_xcopa_commonsense_cause_effect_sw": 60.0, "predict_rougeL_for_task1175_xcopa_commonsense_cause_effect_sw": 60.0, "predict_exact_match_for_task943_copa_mr_commonsense_cause_effect": 54.0, "predict_rouge1_for_task943_copa_mr_commonsense_cause_effect": 54.0, "predict_rougeL_for_task943_copa_mr_commonsense_cause_effect": 54.0, "predict_exact_match_for_task1168_xcopa_commonsense_reasoning_ht": 52.0, "predict_rouge1_for_task1168_xcopa_commonsense_reasoning_ht": 52.0, "predict_rougeL_for_task1168_xcopa_commonsense_reasoning_ht": 52.0, "predict_exact_match_for_task1181_xcopa_commonsense_cause_effect_tr": 57.0, "predict_rouge1_for_task1181_xcopa_commonsense_cause_effect_tr": 57.0, "predict_rougeL_for_task1181_xcopa_commonsense_cause_effect_tr": 57.0, "predict_exact_match_for_task1180_xcopa_commonsense_reasoning_tr": 50.0, "predict_rouge1_for_task1180_xcopa_commonsense_reasoning_tr": 50.0, "predict_rougeL_for_task1180_xcopa_commonsense_reasoning_tr": 50.0, "predict_exact_match_for_task968_xcopa_commonsense_reasoning_et": 53.0, "predict_rouge1_for_task968_xcopa_commonsense_reasoning_et": 53.0, "predict_rougeL_for_task968_xcopa_commonsense_reasoning_et": 53.0, "predict_exact_match_for_task1172_xcopa_commonsense_reasoning_it": 52.0, "predict_rouge1_for_task1172_xcopa_commonsense_reasoning_it": 52.0, "predict_rougeL_for_task1172_xcopa_commonsense_reasoning_it": 52.0, "predict_exact_match_for_task938_copa_hi_commonsense_reasoning": 66.0, "predict_rouge1_for_task938_copa_hi_commonsense_reasoning": 66.0, "predict_rougeL_for_task938_copa_hi_commonsense_reasoning": 66.0, "predict_exact_match_for_task1173_xcopa_commonsense_cause_effect_it": 68.0, "predict_rouge1_for_task1173_xcopa_commonsense_cause_effect_it": 68.0, "predict_rougeL_for_task1173_xcopa_commonsense_cause_effect_it": 68.0, "predict_exact_match_for_task1629_copa_hr_classification": 63.0, "predict_rouge1_for_task1629_copa_hr_classification": 63.0, "predict_rougeL_for_task1629_copa_hr_classification": 63.0, "predict_exact_match_for_task1626_copa_hr_question_answering": 50.0, "predict_rouge1_for_task1626_copa_hr_question_answering": 68.5668, "predict_rougeL_for_task1626_copa_hr_question_answering": 68.5668, "predict_exact_match_for_task1176_xcopa_commonsense_reasoning_ta": 51.0, "predict_rouge1_for_task1176_xcopa_commonsense_reasoning_ta": 51.0, "predict_rougeL_for_task1176_xcopa_commonsense_reasoning_ta": 51.0, "predict_exact_match_for_task1183_xcopa_commonsense_cause_effect_vi": 56.0, "predict_rouge1_for_task1183_xcopa_commonsense_cause_effect_vi": 56.0, "predict_rougeL_for_task1183_xcopa_commonsense_cause_effect_vi": 56.0, "predict_exact_match_for_task464_parsinlu_entailment_sentence_generation": 0.0, "predict_rouge1_for_task464_parsinlu_entailment_sentence_generation": 1.5, "predict_rougeL_for_task464_parsinlu_entailment_sentence_generation": 1.5, "predict_exact_match_for_task940_copa_gu_commonsense_reasoning": 59.0, "predict_rouge1_for_task940_copa_gu_commonsense_reasoning": 59.0, "predict_rougeL_for_task940_copa_gu_commonsense_reasoning": 59.0, "predict_exact_match_for_task1169_xcopa_commonsense_cause_effect_ht": 51.0, "predict_rouge1_for_task1169_xcopa_commonsense_cause_effect_ht": 51.0, "predict_rougeL_for_task1169_xcopa_commonsense_cause_effect_ht": 51.0, "predict_exact_match_for_task534_farstail_entailment": 42.0, "predict_rouge1_for_task534_farstail_entailment": 42.0, "predict_rougeL_for_task534_farstail_entailment": 42.0, "predict_exact_match_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_rouge1_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_rougeL_for_task1178_xcopa_commonsense_reasoning_th": 50.0, "predict_exact_match_for_title_generation": 3.0, "predict_rouge1_for_title_generation": 9.3905, "predict_rougeL_for_title_generation": 9.1048, "predict_exact_match_for_cause_effect_classification": 54.2667, "predict_rouge1_for_cause_effect_classification": 55.2986, "predict_rougeL_for_cause_effect_classification": 55.2986, "predict_exact_match_for_textual_entailment": 21.0, "predict_rouge1_for_textual_entailment": 28.8333, "predict_rougeL_for_textual_entailment": 28.8333, "predict_exact_match_for_answerability_classification": 56.0, "predict_rouge1_for_answerability_classification": 56.0, "predict_rougeL_for_answerability_classification": 56.0, "predict_gen_len": 4.0671, "predict_global_step": 0, "predict_runtime": 595.6016, "predict_samples_per_second": 5.876, "predict_steps_per_second": 1.469, "predict_samples": 3500}